# 本地测试操作手册（边牧级）

本手册提供在本地环境（Windows/Linux）使用 Docker 执行数据集测试的完整流程。
请注意每条命令的执行环境（PowerShell/CMD 或 bash）

## 目录

1. [环境准备](#1-环境准备)
2. [环境检查](#2-环境检查)
3. [准备项目代码](#3-准备项目代码)
4. [构建 Docker 镜像](#4-构建-docker-镜像)
5. [启动容器](#5-启动容器)
6. [准备模型和数据](#6-准备模型和数据)
7. [执行测试](#7-执行测试)
8. [查看结果](#8-查看结果)
9. [常见问题](#9-常见问题)

---

## 1. 环境准备

### 1.1 系统要求

- **操作系统**: Windows 10/11 或 Linux (Ubuntu 20.04+)
- **Docker**: Docker Desktop (Windows) 或 Docker Engine (Linux)
- **GPU**: NVIDIA GPU（推荐 8GB+ 显存，支持 CUDA 12.4）
- **内存**: 16GB+ RAM
- **磁盘空间**: 至少 50GB 可用空间（用于镜像、模型和数据集）

### 1.2 安装 Docker

**Windows**:
1. 下载并安装 [Docker Desktop for Windows](https://www.docker.com/products/docker-desktop)
2. 安装时确保启用 WSL 2 后端（推荐）
3. 启动 Docker Desktop

**Linux**:
```bash
# Ubuntu/Debian
curl -fsSL https://get.docker.com | sh

# 启动 Docker 服务
sudo systemctl start docker
sudo systemctl enable docker

# 将当前用户添加到 docker 组（可选，避免每次使用 sudo）
sudo usermod -aG docker $USER
# 重新登录后生效
```

---

## 2. 环境检查

### 2.1 检查 Docker

**Windows (PowerShell)**:
```powershell
# 检查 Docker 版本
docker --version

# 检查 Docker 服务状态
docker info
```

**Linux**:
```bash
# 检查 Docker 版本
docker --version

# 检查 Docker 服务状态
sudo systemctl status docker
```

### 2.2 检查 NVIDIA Container Toolkit

**Windows**:
- Docker Desktop 会自动处理 GPU 支持（如果已安装 NVIDIA 驱动）
- 确保已安装最新的 NVIDIA 驱动

**Linux**:
```bash
# 检查是否已安装
docker run --rm --gpus all nvidia/cuda:12.4.0-base-ubuntu22.04 nvidia-smi

# 如果失败，安装 nvidia-container-toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

### 2.3 检查 GPU

**Windows (PowerShell)**:
```powershell
# 查看 GPU 信息
nvidia-smi

# 验证 Docker GPU 访问
docker run --rm --gpus all nvidia/cuda:12.4.0-base-ubuntu22.04 nvidia-smi
```

**Linux**:
```bash
# 查看 GPU 信息
nvidia-smi

# 验证 Docker GPU 访问
docker run --rm --gpus all nvidia/cuda:12.4.0-base-ubuntu22.04 nvidia-smi
```

### 2.4 检查 Git（可选）

```bash
# Windows/Linux
git --version

# 如果未安装，Windows 从 https://git-scm.com/download/win 下载
# Linux: sudo apt install git
```

---

## 3. 准备项目代码

### 3.1 方式一：Git 克隆（推荐）

**Windows (PowerShell)**:
```powershell
# 进入工作目录
cd D:\Projects  # 或其他目录

# 克隆项目
git clone https://github.com/YANHAN-BLCU/NeuroLens.git

cd NeuroLens
```

**Linux**:
```bash
# 进入工作目录
cd ~/Projects  # 或其他目录

# 克隆项目
git clone https://github.com/YANHAN-BLCU/NeuroLens.git

cd NeuroLens
```

### 3.2 方式二：直接下载 ZIP

1. 访问 https://github.com/YANHAN-BLCU/NeuroLens
2. 点击 "Code" -> "Download ZIP"
3. 解压到本地目录

### 3.3 验证代码

**Windows (PowerShell)**:
```powershell
cd D:\Projects\NeuroLens
dir
# 应看到: docker/, scripts/, engine/, requirements.txt 等
```

**Linux**:
```bash
cd ~/Projects/NeuroLens
ls -la
# 应看到: docker/, scripts/, engine/, requirements.txt 等
```

---

## 4. 构建 Docker 镜像

**Windows (PowerShell)**:
```powershell
cd D:\Projects\NeuroLens

# 构建镜像（镜像名称: neurolens:v1.0）
docker build -t neurolens:v1.0 -f docker/Dockerfile .

# 验证镜像
docker images | Select-String neurolens
```

**Linux**:
```bash
cd ~/Projects/NeuroLens

# 构建镜像（镜像名称: neurolens:v1.0）
docker build -t neurolens:v1.0 -f docker/Dockerfile .

# 验证镜像
docker images | grep neurolens
```

**预计时间**: 5-15 分钟（取决于网络速度，已优化构建上下文，不会传输模型文件）

**注意**: 由于已配置 `.dockerignore`，构建时不会包含 `ms_models/` 目录，构建速度会很快。

---

## 5. 启动容器

### 5.1 准备环境变量

**Windows (PowerShell)**:
```powershell
# 设置 ModelScope Token（如果使用 ModelScope）
$env:MODELSCOPE_TOKEN="ms-d9625645-acb2-444f-b0df-6685dfc743b5"

# 或创建 .env 文件（可选）
echo "MODELSCOPE_TOKEN=ms-d9625645-acb2-444f-b0df-6685dfc743b5" > .env
```

**Linux**:
```bash
# 设置 ModelScope Token（如果使用 ModelScope）
export MODELSCOPE_TOKEN=ms-d9625645-acb2-444f-b0df-6685dfc743b5

# 或创建 .env 文件（可选）
echo "MODELSCOPE_TOKEN=ms-d9625645-acb2-444f-b0df-6685dfc743b5" > .env
```

### 5.2 启动容器

**Windows (PowerShell)**:
```powershell
cd D:\Projects\NeuroLens

docker run --gpus all -it `
  --name neurolens `
  -v ${PWD}:/workspace `
  -v D:\DockerCache:/workspace/.cache `
  -e MODELSCOPE_TOKEN=$env:MODELSCOPE_TOKEN `
  neurolens:v1.0 `
  /bin/bash
```

**Linux**:
```bash
cd ~/Projects/NeuroLens

docker run --gpus all -it \
  --name neurolens \
  -v $(pwd):/workspace \
  -v ~/docker-cache:/workspace/.cache \
  -e MODELSCOPE_TOKEN=${MODELSCOPE_TOKEN} \
  neurolens:v1.0 \
  /bin/bash
```

**参数说明**:
- `--gpus all`: 启用所有 GPU
- `-it`: 交互式终端
- `--name neurolens`: 容器名称
- `-v $(pwd):/workspace`: 挂载项目目录（Windows 使用 `${PWD}`）
- `-v ~/docker-cache:/workspace/.cache`: 挂载缓存目录（Windows 使用 `D:\DockerCache`）
- `-e MODELSCOPE_TOKEN`: 传递 ModelScope Token

### 5.3 验证容器

```bash
# 在容器内执行
nvidia-smi
python -c "import torch; print('CUDA available:', torch.cuda.is_available())"
```

---

## 6. 准备模型和数据

### 6.1 检查模型路径

```bash
# 在容器内执行
cd /workspace

# 检查模型是否存在
ls -la /workspace/ms_models/LLM-Research/

# 应看到:
# - Meta-Llama-3-8B-Instruct/
# - Llama-Guard-3-8B/
```

### 6.2 如果模型不存在，下载模型

```bash
# 在容器内执行
export MODELSCOPE_TOKEN=ms-d9625645-acb2-444f-b0df-6685dfc743b5

# 下载模型
python scripts/download_models.py --all-8b --output /workspace/ms_models
```

**注意**: 模型较大（约 16GB），下载需要时间。

### 6.3 准备 SALAD 数据集

```bash
# 在容器内执行
cd /workspace

# 下载数据集（如果未下载）
python scripts/download_salad.py

# 验证数据集
ls -la /workspace/data/salad/raw/
# 应看到: base_set_train.jsonl, attack_enhanced_set_train.jsonl 等
```

---

## 7. 执行测试

### 7.1 快速测试（10 个样本）

只测试一个也行，能跑出来就行

```bash
# 在容器内执行
cd /workspace

python scripts/evaluate_salad_pipeline.py \
  --data_dir /workspace/data/salad/raw \
  --output /workspace/logs/test_10.jsonl \
  --config base_set \
  --max_samples 10
```

### 7.2 完整测试（base_set，21,318 个样本）

```bash
# 在容器内执行
cd /workspace

# 创建输出目录
mkdir -p /workspace/logs

# 运行评估（建议使用 nohup 后台运行）
nohup python scripts/evaluate_salad_pipeline.py \
  --data_dir /workspace/data/salad/raw \
  --output /workspace/logs/base_set.jsonl \
  --config base_set \
  > /workspace/logs/base_set.log 2>&1 &

# 查看进度
tail -f /workspace/logs/base_set.log
```

### 7.3 其他配置测试

```bash
# attack_enhanced_set (5,000 样本)
python scripts/evaluate_salad_pipeline.py \
  --data_dir /workspace/data/salad/raw \
  --output /workspace/logs/attack_enhanced_set.jsonl \
  --config attack_enhanced_set

# defense_enhanced_set (200 样本)
python scripts/evaluate_salad_pipeline.py \
  --data_dir /workspace/data/salad/raw \
  --output /workspace/logs/defense_enhanced_set.jsonl \
  --config defense_enhanced_set

# mcq_set (3,840 样本)
python scripts/evaluate_salad_pipeline.py \
  --data_dir /workspace/data/salad/raw \
  --output /workspace/logs/mcq_set.jsonl \
  --config mcq_set
```

### 7.4 断点续传

```bash
# 从第 1000 个样本开始继续
python scripts/evaluate_salad_pipeline.py \
  --data_dir /workspace/data/salad/raw \
  --output /workspace/logs/base_set.jsonl \
  --config base_set \
  --start_from 1000
```

---

## 8. 查看结果

### 8.1 查看日志

```bash
# 在容器内执行
tail -f /workspace/logs/base_set.log

# 查看最后 100 行
tail -n 100 /workspace/logs/base_set.log
```

### 8.2 查看结果文件

```bash
# 在容器内执行
# 查看结果行数
wc -l /workspace/logs/base_set.jsonl

# 查看最后几条结果
tail -n 5 /workspace/logs/base_set.jsonl | python -m json.tool
```

### 8.3 分析结果（可选）（没必要）

```bash
# 在容器内执行
python scripts/analyze_salad_results.py \
  --input /workspace/logs/base_set.jsonl
```

### 8.4 从容器复制结果到宿主机

**Windows (PowerShell)**:
```powershell
# 在宿主机执行（退出容器后）
docker cp neurolens:/workspace/logs/base_set.jsonl .\logs\
```

**Linux**:
```bash
# 在宿主机执行（退出容器后）
docker cp neurolens:/workspace/logs/base_set.jsonl ./logs/
```

---

## 9. 常见问题（非常不常见，但是总会发生）

### 9.1 CUDA 不可用

**症状**: `torch.cuda.is_available()` 返回 `False`

**解决方案**:
```bash
# 1. 检查容器是否使用 --gpus all 启动
# Windows
docker inspect neurolens | Select-String -Pattern "gpu" -CaseSensitive:$false

# Linux
docker inspect neurolens | grep -i gpu

# 2. 检查 nvidia-smi 在容器内是否可用
docker exec neurolens nvidia-smi

# 3. 如果不可用，重新启动容器
docker stop neurolens
docker rm neurolens
# 然后使用 --gpus all 重新启动（见步骤 5.2）
```

### 9.2 显存不足（应该不用考虑）

**症状**: `CUDA out of memory`

**解决方案**:
- 模型已启用 4-bit 量化，如果仍不足，考虑：
  - 使用更小的 `max_tokens` 参数
  - 关闭其他占用 GPU 的程序
  - 使用 CPU 模式（会很慢）

### 9.3 模型路径找不到

**症状**: `FileNotFoundError: Model not found`

**解决方案**:
```bash
# 在容器内检查模型路径
python scripts/check_models.py

# 如果模型在宿主机，确保挂载正确
# 启动容器时添加: -v /host/path/to/models:/workspace/ms_models
```

### 9.4 数据集路径错误

**症状**: `Data directory not found`

**解决方案**:
```bash
# 检查数据集是否存在
ls -la /workspace/data/salad/raw/

# 如果不存在，下载数据集
python scripts/download_salad.py
```

### 9.5 容器退出后重新进入

**Windows (PowerShell)**:
```powershell
# 查看容器状态
docker ps -a | Select-String neurolens

# 如果容器已停止，启动它
docker start neurolens

# 进入容器
docker exec -it neurolens /bin/bash
```

**Linux**:
```bash
# 查看容器状态
docker ps -a | grep neurolens

# 如果容器已停止，启动它
docker start neurolens

# 进入容器
docker exec -it neurolens /bin/bash
```

### 9.6 Docker 构建缓慢

**症状**: 构建镜像时传输大量文件

**解决方案**:
- 确保 `.dockerignore` 文件存在并正确配置
- 检查是否排除了 `ms_models/` 目录
- 构建上下文应该只有几 MB，而不是几十 GB

### 9.7 Windows 路径问题

**症状**: 挂载路径错误或找不到文件

**解决方案**:
```powershell
# Windows 使用 ${PWD} 或完整路径
# 确保使用正斜杠或双反斜杠
docker run --gpus all -it `
  --name neurolens `
  -v D:\Projects\NeuroLens:/workspace `
  ...
```

---

## 快速参考

### 常用命令

**Windows (PowerShell)**:
```powershell
# 进入容器
docker exec -it neurolens /bin/bash

# 查看 GPU 使用情况
nvidia-smi

# 查看容器日志
docker logs neurolens

# 停止容器
docker stop neurolens

# 删除容器
docker rm neurolens

# 查看容器状态
docker ps -a | Select-String neurolens
```

**Linux**:
```bash
# 进入容器
docker exec -it neurolens /bin/bash

# 查看 GPU 使用情况
nvidia-smi

# 查看容器日志
docker logs neurolens

# 停止容器
docker stop neurolens

# 删除容器
docker rm neurolens

# 查看容器状态
docker ps -a | grep neurolens
```

### 文件路径

- **项目代码**: `/workspace/` (容器内)
- **模型路径**: `/workspace/ms_models/LLM-Research/` (容器内)
- **数据集路径**: `/workspace/data/salad/raw/` (容器内)
- **输出结果**: `/workspace/logs/` (容器内)
- **缓存目录**: `/workspace/.cache/` (容器内，挂载到宿主机)

---

## 不使用 Docker（可选）

如果你不想使用 Docker，可以直接在本地 Python 环境中运行。参考以下步骤：

### 安装本地 Python 环境

**Windows**:
```powershell
# 下载并安装 Python 3.11
# 从 https://www.python.org/downloads/ 下载安装包
# 安装时勾选 "Add Python to PATH"

# 创建虚拟环境
python -m venv venv
.\venv\Scripts\Activate.ps1

# 安装依赖
pip install -r requirements.txt
```

**Linux**:
```bash
# 安装 Python 3.11
sudo apt update
sudo apt install python3.11 python3.11-venv python3-pip

# 创建虚拟环境
python3.11 -m venv venv
source venv/bin/activate

# 安装依赖
pip install -r requirements.txt
```

### 运行测试

```bash
# 设置环境变量
export MODELSCOPE_TOKEN=ms-d9625645-acb2-444f-b0df-6685dfc743b5

# 运行测试
python scripts/evaluate_salad_pipeline.py \
  --data_dir ./data/salad/raw \
  --output ./logs/test_10.jsonl \
  --config base_set \
  --max_samples 10
```

**注意**: 本地环境需要手动管理 Python 版本、依赖和 CUDA 配置，Docker 方式更简单可靠。

---

**完成以上步骤后，即可在本地环境使用 Docker 执行数据集测试。如有问题，请参考常见问题部分或查看项目文档。**
